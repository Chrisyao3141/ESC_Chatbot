/home/yaoziqi/miniforge3/envs/lab/lib/python3.12/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Model Loaded
Training Data Loaded
Beginning training
tensor(2.4325, device='cuda:3', grad_fn=<NllLossBackward0>)
Epoch: 0, Epoch steps: 1000, Global steps: 1000, Time taken: 0.18598175048828125 seconds, Loss: 2.4324569702148438
Traceback (most recent call last):
  File "/home/yaoziqi/Projects/Learning/NLP/ESC_Chatbot/main.py", line 143, in <module>
    results = train(model, optimizer)
              ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/yaoziqi/Projects/Learning/NLP/ESC_Chatbot/main.py", line 77, in train
    optimizer.step()
  File "/home/yaoziqi/miniforge3/envs/lab/lib/python3.12/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/yaoziqi/miniforge3/envs/lab/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/yaoziqi/miniforge3/envs/lab/lib/python3.12/site-packages/transformers/optimization.py", line 484, in step
    exp_avg.mul_(beta1).add_(grad, alpha=(1.0 - beta1))
    ^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
